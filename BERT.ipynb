{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f59b17-3f5d-4366-aa93-e593f54ea6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "files = zipfile.ZipFile('atepc_datasets.zip','r')\n",
    "files.extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2487ce-ca68-4d47-9d8c-4f60ad6eae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from pytorch_transformers.modeling_bert import BertModel, BertForTokenClassification, BertPooler, BertSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee157e48-cceb-4c14-bc7e-4451b160ca1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def readfile(filename):\\n    f = open(filename, encoding=\\'utf8\\')\\n    data = []\\n    sentence = []\\n    tag= []\\n    polarity = []\\n    for line in f:\\n        if len(line)==0 or line.startswith(\\'-DOCSTART\\') or line[0 ]==\"\\n\":\\n            if len(sentence) > 0:\\n                data.append((sentence, tag, polarity))\\n                sentence = []\\n                tag = []\\n                polarity = []\\n            continue\\n        splits = line.split(\\' \\')\\n        if len(splits) != 3:\\n            print(\\'warning! detected error line(s) in input file:{}\\'.format(line))\\n        sentence.append(splits[0])\\n        tag.append(splits[-2])\\n        polarity.append(int(splits[-1][:-1]))\\n\\n    if len(sentence) > 0:\\n        data.append((sentence, tag, polarity))\\n    return data\\n\\n##############\\n#改数据集\\n##############\\n#camera car laptops mixed notebook phone restaurant twitter\\ntrain_data = readfile(\"./atepc_datasets/camera/camera.atepc.train.dat\")\\nrandom.shuffle(train_data)\\ntest_data = readfile(\"./atepc_datasets/camera/camera.atepc.test.dat\")\\nrandom.shuffle(test_data)\\nprint(\"训练集数量：%d 测试集数量：%d\" % (len(train_data), len(test_data)))\\nprint(\"实例：\")\\nprint(train_data[0])'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, encoding='utf8')\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tag= []\n",
    "    polarity = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0 ]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, tag, polarity))\n",
    "                sentence = []\n",
    "                tag = []\n",
    "                polarity = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        if len(splits) != 3:\n",
    "            print('warning! detected error line(s) in input file:{}'.format(line))\n",
    "        sentence.append(splits[0])\n",
    "        tag.append(splits[-2])\n",
    "        polarity.append(int(splits[-1][:-1]))\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, tag, polarity))\n",
    "    return data\n",
    "\n",
    "##############\n",
    "#改数据集\n",
    "##############\n",
    "#camera car laptops mixed notebook phone restaurant twitter\n",
    "train_data = readfile(\"./atepc_datasets/camera/camera.atepc.train.dat\")\n",
    "random.shuffle(train_data)\n",
    "test_data = readfile(\"./atepc_datasets/camera/camera.atepc.test.dat\")\n",
    "random.shuffle(test_data)\n",
    "print(\"训练集数量：%d 测试集数量：%d\" % (len(train_data), len(test_data)))\n",
    "print(\"实例：\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0914a9fc-1e8e-4151-b9a1-8df9ab76acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量：1202 测试集数量：323\n",
      "实例：\n",
      "(['商', '业', '拍', '摄', '比', '较', '方', '便'], ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'O', 'O', 'O', 'O'], [2, 2, 2, 2, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename, encoding='utf8')\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tag = []\n",
    "    polarity = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0 and all(polarity): # 筛选掉没有标签信息的数据\n",
    "                data.append((sentence, tag, polarity))\n",
    "            sentence = []\n",
    "            tag = []\n",
    "            polarity = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        if len(splits) != 3:\n",
    "            print('warning! detected error line(s) in input file:{}'.format(line))\n",
    "        sentence.append(splits[0])\n",
    "        tag.append(splits[-2])\n",
    "        polarity.append(int(splits[-1][:-1]))\n",
    "\n",
    "    if len(sentence) > 0 and all(polarity): # 筛选掉没有标签信息的数据\n",
    "        data.append((sentence, tag, polarity))\n",
    "    return data\n",
    "\n",
    "#改数据集\n",
    "##############\n",
    "#camera car laptops mixed notebook phone restaurant twitter\n",
    "train_data = readfile(\"./atepc_datasets/camera/camera.atepc.train.dat\")\n",
    "random.shuffle(train_data)\n",
    "test_data = readfile(\"./atepc_datasets/camera/camera.atepc.test.dat\")\n",
    "random.shuffle(test_data)\n",
    "print(\"训练集数量：%d 测试集数量：%d\" % (len(train_data), len(test_data)))\n",
    "print(\"实例：\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5752ec5d-2dbd-4ae9-a5d5-9d3cb6344b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-0\n",
      "['商', '业', '拍', '摄', '比', '较', '方', '便']\n",
      "['商', '业', '拍', '摄']\n",
      "['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'O', 'O', 'O', 'O']\n",
      "['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']\n",
      "[2, 2, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b=None, sentence_label=None, aspect_label=None, polarity=None):\n",
    "        self.guid = guid  # 输入数据的id\n",
    "        self.text_a = text_a # 输入的句子\n",
    "        self.text_b = text_b # 句子中的aspect\n",
    "        self.sentence_label = sentence_label # 句子标注\n",
    "        self.aspect_label = aspect_label # text_b的标注\n",
    "        self.polarity = polarity # 情感倾向\n",
    "        \n",
    "def create_example(lines, set_type):\n",
    "    examples = []\n",
    "    for i, (sentence, tag, polarity) in enumerate(lines):\n",
    "        aspect = []\n",
    "        aspect_tag = []\n",
    "        aspect_polarity = [-1]\n",
    "        for w, t, p in zip(sentence, tag, polarity):\n",
    "            if p != -1:\n",
    "                aspect.append(w)\n",
    "                aspect_tag.append(t)\n",
    "                aspect_polarity.append(-1)\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        text_a = sentence\n",
    "        text_b = aspect\n",
    "        polarity.extend(aspect_polarity)\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, sentence_label=tag, \n",
    "                         aspect_label=aspect_tag, polarity=polarity))\n",
    "    return examples\n",
    "\n",
    "train_examples = create_example(train_data, \"train\")\n",
    "test_examples = create_example(test_data, \"test\")\n",
    "#展示\n",
    "print(train_examples[0].guid)\n",
    "print(train_examples[0].text_a)\n",
    "print(train_examples[0].text_b)\n",
    "print(train_examples[0].sentence_label)\n",
    "print(train_examples[0].aspect_label)\n",
    "print(train_examples[0].polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f364383a-1725-4496-b2f4-2c9845d6a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH = 80 #最大句子长度\n",
    "LABEL_LIST = [\"O\", \"B-ASP\", \"I-ASP\", \"[CLS]\", \"[SEP]\"] \n",
    "PRETRAINED_BERT_MODEL = \"bert-base-chinese\"\n",
    "NUM_LABELS = len(LABEL_LIST) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b87870-3330-4611-806a-40ac03b88e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL, do_lower_case=True) #分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dec920d-9e58-45ab-b62e-ac6e9d6ec2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_polarity(examples):\n",
    "    for i in range(len(examples)):\n",
    "        polarities = []\n",
    "        for polarity in examples[i].polarity:\n",
    "            if polarity == 2:\n",
    "                polarities.append(1)\n",
    "            else:\n",
    "                polarities.append(polarity)\n",
    "        examples[i].polarity = polarities\n",
    "    return examples\n",
    "\n",
    "train_examples = convert_polarity(train_examples)\n",
    "test_examples = convert_polarity(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fbc019-346a-452a-addd-3afaf11b550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [00:00<00:00, 2308.29it/s]\n",
      "100%|██████████| 323/323 [00:00<00:00, 1603.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1555, 689, 2864, 3029, 3683, 6772, 3175, 912, 102, 1555, 689, 2864, 3029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[4, 2, 3, 3, 3, 1, 1, 1, 1, 5, 2, 3, 3, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids_spc, input_mask, segment_ids, label_id, \n",
    "                 polarities=None, valid_ids=None, label_mask=None):\n",
    "        self.input_ids_spc = input_ids_spc\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "        self.polarities = polarities\n",
    "    \n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
    "\n",
    "    features = []\n",
    "    for example in tqdm.tqdm(examples):\n",
    "        text_spc_tokens = example.text_a\n",
    "        aspect_tokens = example.text_b\n",
    "        sentence_label = example.sentence_label\n",
    "        aspect_label = example.aspect_label\n",
    "        polaritiylist = example.polarity\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        polarities = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        text_spc_tokens.extend(['[SEP]'])\n",
    "        text_spc_tokens.extend(aspect_tokens)  # 将输入文本（text_a）和识别出来的实体(text_b)连接起来\n",
    "        enum_tokens = text_spc_tokens\n",
    "        sentence_label.extend(['[SEP]'])\n",
    "        # sentence_label.extend(['O'])\n",
    "        sentence_label.extend(aspect_label)\n",
    "        label_lists = sentence_label\n",
    "        for i, word in enumerate(enum_tokens):  # 为文本和实体生成标签序列\n",
    "            token = tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            label_1 = label_lists[i]\n",
    "            polarity_1 = polaritiylist[i]\n",
    "            for m in range(len(token)):  # 一个词中不同字，只在首字上标注\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    polarities.append(polarity_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        if len(tokens) >= max_seq_length - 1:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "            polarities = polarities[0:(max_seq_length - 2)]\n",
    "            labels = labels[0:(max_seq_length - 2)]\n",
    "            valid = valid[0:(max_seq_length - 2)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
    "        ntokens = []\n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        ntokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.insert(0, 1)\n",
    "        label_mask.insert(0, 1)\n",
    "        label_ids.append(label_map[\"[CLS]\"])\n",
    "        for i, token in enumerate(tokens): \n",
    "            ntokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        valid.append(1)\n",
    "        label_mask.append(1)\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        input_ids_spc = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        input_mask = [1] * len(input_ids_spc)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "        # 将各属性补齐\n",
    "        while len(input_ids_spc) < max_seq_length:\n",
    "            input_ids_spc.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(1)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        while len(polarities) < max_seq_length:\n",
    "            polarities.append(-1)\n",
    "        assert len(input_ids_spc) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids_spc=input_ids_spc,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_ids,\n",
    "                          polarities=polarities,\n",
    "                          valid_ids=valid,\n",
    "                          label_mask=label_mask))\n",
    "    return features\n",
    "\n",
    "\n",
    "train_features = convert_examples_to_features(train_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_features(test_examples, LABEL_LIST, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "#展示\n",
    "print(train_features[0].input_ids_spc)\n",
    "print(train_features[0].input_mask)\n",
    "print(train_features[0].segment_ids)\n",
    "print(train_features[0].label_id)\n",
    "print(train_features[0].valid_ids)\n",
    "print(train_features[0].label_mask)\n",
    "print(train_features[0].polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a18483e2-b3d1-490f-9076-7993bcd81259",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#超参数设置#\n",
    "############\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 2 #16\n",
    "#DEVICE = \"cpu\"\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa36d41-a8e8-42d2-8635-0dc862aff980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_base_model = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n",
    "bert_base_model.config.num_labels = NUM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67534b0b-6f6f-43f0-a54f-70113a9723c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = config\n",
    "        self.SA = BertSelfAttention(config)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_vec = np.zeros((inputs.size(0), 1, 1, MAX_SEQUENCE_LENGTH))\n",
    "        zero_tensor = torch.tensor(zero_vec).float().to(DEVICE)\n",
    "        SA_out = self.SA(inputs, zero_tensor)\n",
    "        return self.tanh(SA_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81573f1-8d5e-46c9-a3cc-002d90a9c9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(BertForTokenClassification):\n",
    "    def __init__(self, bert_base_model):\n",
    "        config = bert_base_model.config\n",
    "        super(Model, self).__init__(config=config)\n",
    "        self.bert_for_global_context = bert_base_model  # BERT编码器\n",
    "        self.bert_for_local_context = bert_base_model\n",
    "        self.pooler = BertPooler(config)  # 池化层\n",
    "        self.dense = torch.nn.Linear(768, 3)  # 全连接层\n",
    "        self.bert_global_focus = self.bert_for_global_context\n",
    "        self.dropout = torch.nn.Dropout(0.1)  # dropout层\n",
    "        self.SA1 = SelfAttention(config)  # 自注意力机制\n",
    "        self.SA2 = SelfAttention(config)\n",
    "        self.linear_double = torch.nn.Linear(768 * 2, 768)  # 全连接层\n",
    "        self.linear_triple = torch.nn.Linear(768 * 3, 768)\n",
    "        \n",
    "    def get_ids_for_local_context_extractor(self, text_indices):\n",
    "        text_ids = text_indices.detach().cpu().numpy()\n",
    "        for text_i in range(len(text_ids)):\n",
    "            sep_index = np.argmax((text_ids[text_i] == 102))\n",
    "            text_ids[text_i][sep_index + 1:] = 0\n",
    "        return torch.tensor(text_ids).to(DEVICE)\n",
    "\n",
    "    def get_batch_token_labels_bert_base_indices(self, labels):\n",
    "        if labels is None:\n",
    "            return\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        for text_i in range(len(labels)):\n",
    "            sep_index = np.argmax((labels[text_i] == 5))\n",
    "            labels[text_i][sep_index + 1:] = 0\n",
    "        return torch.tensor(labels).to(DEVICE)\n",
    "\n",
    "    def get_batch_polarities(self, b_polarities):\n",
    "        b_polarities = b_polarities.detach().cpu().numpy()\n",
    "        shape = b_polarities.shape\n",
    "        polarities = np.zeros((shape[0]))\n",
    "        i = 0\n",
    "        for polarity in b_polarities:\n",
    "            polarity_idx = np.flatnonzero(polarity + 1)\n",
    "            polarities[i] = polarity[polarity_idx[0]]\n",
    "            i += 1\n",
    "        polarities = torch.from_numpy(polarities).long().to(DEVICE)\n",
    "        return polarities\n",
    "    \n",
    "    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarities=None, valid_ids=None, attention_mask_label=None):\n",
    "        input_ids_spc = self.get_ids_for_local_context_extractor(input_ids_spc)\n",
    "        labels = self.get_batch_token_labels_bert_base_indices(labels)\n",
    "        global_context_out, _ = self.bert_for_global_context(input_ids_spc, token_type_ids, attention_mask)\n",
    "        polarity_labels = self.get_batch_polarities(polarities)\n",
    "        batch_size, max_len, feat_dim = global_context_out.shape\n",
    "        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32).to(DEVICE)\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                if valid_ids[i][j].item() == 1:\n",
    "                    jj += 1\n",
    "                    global_valid_output[i][jj] = global_context_out[i][j]\n",
    "        global_context_out = self.dropout(global_valid_output)\n",
    "        ate_logits = self.classifier(global_context_out)\n",
    "        pooled_out = self.pooler(global_context_out)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        apc_logits = self.dense(pooled_out)\n",
    "        if labels is not None:\n",
    "            # 训练过程计算损失\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "            loss_sen = torch.nn.CrossEntropyLoss()\n",
    "            loss_ate = loss_fct(ate_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss_apc = loss_sen(apc_logits, polarity_labels)\n",
    "            return loss_ate, loss_apc\n",
    "        else:\n",
    "            return ate_logits, apc_logits\n",
    "\n",
    "model = Model(bert_base_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23094b73-574e-44ea-963a-865b0d0cdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad21dcaa-94f7-4f57-87dd-b8a8ae1a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率衰减\n",
    "param_optimizer = list(model.named_parameters())  # 模型中的所有参数\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.00001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.00001}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "464ed476-f8d7-4afe-bebb-e394d2937131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30218057-337f-4637-9de3-030135da6f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_polarities = torch.tensor([f.polarities for f in test_features], dtype=torch.long)\n",
    "all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)\n",
    "all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_spc_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids)\n",
    "eval_sampler = RandomSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de5dc30-5921-4f26-ab67-c037f0ec77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#import logging\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "##########\n",
    "#epoch#\n",
    "##########\n",
    "EPOCH = 500  # 共计算5个epoch\n",
    "EVAL_STEP = 10  # 每10个step执行一个评估\n",
    "\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.INFO)\n",
    "#logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767f32e0-c830-4b78-9292-31c4e9dbcc7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, label_list):\n",
    "    apc_result = {'max_apc_test_acc': 0, 'max_apc_test_f1': 0}\n",
    "    ate_result = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    n_test_correct, n_test_total = 0, 0\n",
    "    test_apc_logits_all, test_polarities_all = None, None\n",
    "    model.eval()  # 将网络设置为评估的状态\n",
    "    label_map = {i: label for i, label in enumerate(label_list, 1)}\n",
    "    for input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask in dataloader:\n",
    "        input_ids_spc = input_ids_spc.to(DEVICE)\n",
    "        input_mask = input_mask.to(DEVICE)\n",
    "        segment_ids = segment_ids.to(DEVICE)\n",
    "        valid_ids = valid_ids.to(DEVICE)\n",
    "        label_ids = label_ids.to(DEVICE)\n",
    "        polarities = polarities.to(DEVICE)\n",
    "        l_mask = l_mask.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            ate_logits, apc_logits = model(\n",
    "                input_ids_spc, segment_ids, input_mask, \n",
    "                valid_ids=valid_ids, polarities=polarities, attention_mask_label=l_mask)\n",
    "        polarities = model.get_batch_polarities(polarities)\n",
    "        n_test_correct += (torch.argmax(apc_logits, -1) == polarities).sum().item()\n",
    "        n_test_total += len(polarities)\n",
    "        if test_polarities_all is None:\n",
    "            test_polarities_all = polarities\n",
    "            test_apc_logits_all = apc_logits\n",
    "        else:\n",
    "            test_polarities_all = torch.cat((test_polarities_all, polarities), dim=0)\n",
    "            test_apc_logits_all = torch.cat((test_apc_logits_all, apc_logits), dim=0)\n",
    "        label_ids = model.get_batch_token_labels_bert_base_indices(label_ids)\n",
    "        ate_logits = torch.argmax(F.log_softmax(ate_logits, dim=2), dim=2)\n",
    "        ate_logits = ate_logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        input_mask = input_mask.to('cpu').numpy()\n",
    "        for i, label in enumerate(label_ids):\n",
    "            temp_1 = []\n",
    "            temp_2 = []\n",
    "            for j, m in enumerate(label):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                elif label_ids[i][j] == len(label_list):\n",
    "                    y_true += temp_1\n",
    "                    y_pred += temp_2\n",
    "                    break\n",
    "                else:\n",
    "                    temp_1.append(label_map.get(label_ids[i][j], 'O'))\n",
    "                    temp_2.append(label_map.get(ate_logits[i][j], 'O'))\n",
    "    test_acc = n_test_correct / n_test_total\n",
    "    test_f1 = f1_score(torch.argmax(test_apc_logits_all, -1).cpu(), test_polarities_all.cpu(),labels=[0, 1], average='macro',zero_division=1)\n",
    "    test_acc = round(test_acc * 100, 2)\n",
    "    test_f1 = round(test_f1 * 100, 2)\n",
    "    apc_result = {'max_apc_test_acc': test_acc, 'max_apc_test_f1': test_f1}    \n",
    "    report = classification_report(y_true, y_pred, digits=4,zero_division=1)\n",
    "    tmps = report.split()\n",
    "    ate_result = round(float(tmps[7]) * 100, 2)\n",
    "    return apc_result, ate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f646e45b-70f7-4a3b-8e05-79a336874dab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/envs/DeepL/lib/python3.9/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m EVAL_STEP \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 评估\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     apc_result, ate_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL_LIST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m apc_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_apc_test_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m max_apc_test_acc:\n\u001b[1;32m     25\u001b[0m         max_apc_test_acc \u001b[38;5;241m=\u001b[39m apc_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_apc_test_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, label_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m l_mask \u001b[38;5;241m=\u001b[39m l_mask\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m     ate_logits, apc_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids_spc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolarities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolarities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m polarities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_batch_polarities(polarities)\n\u001b[1;32m     23\u001b[0m n_test_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(apc_logits, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m polarities)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, input_ids_spc, token_type_ids, attention_mask, labels, polarities, valid_ids, attention_mask_label)\u001b[0m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_token_labels_bert_base_indices(labels)\n\u001b[1;32m     47\u001b[0m global_context_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_for_global_context(input_ids_spc, token_type_ids, attention_mask)\n\u001b[0;32m---> 48\u001b[0m polarity_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_polarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolarities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m batch_size, max_len, feat_dim \u001b[38;5;241m=\u001b[39m global_context_out\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     50\u001b[0m global_valid_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, max_len, feat_dim, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mModel.get_batch_polarities\u001b[0;34m(self, b_polarities)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polarity \u001b[38;5;129;01min\u001b[39;00m b_polarities:\n\u001b[1;32m     38\u001b[0m     polarity_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(polarity \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     polarities[i] \u001b[38;5;241m=\u001b[39m polarity[\u001b[43mpolarity_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     40\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m polarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(polarities)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "max_apc_test_acc = 0\n",
    "max_apc_test_f1 = 0\n",
    "max_ate_test_f1 = 0\n",
    "global_step = 0\n",
    "for epoch in range(EPOCH):\n",
    "    # 每个epoch\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 一个step\n",
    "        model.train()  # 将网络设置为train的模式\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids_spc, input_mask, segment_ids, label_ids, polarities, valid_ids, l_mask = batch  # 取一个batch的数据\n",
    "        loss_ate, loss_apc = model(\n",
    "            input_ids_spc, segment_ids, input_mask, label_ids, polarities, valid_ids, l_mask)  # 前向传播，计算损失\n",
    "        loss = loss_ate + loss_apc \n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        nb_tr_examples += input_ids_spc.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        if global_step % EVAL_STEP == 0:  # 评估\n",
    "            apc_result, ate_result = evaluate(eval_dataloader, LABEL_LIST)\n",
    "            if apc_result['max_apc_test_acc'] > max_apc_test_acc:\n",
    "                max_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            if apc_result['max_apc_test_f1'] > max_apc_test_f1:\n",
    "                max_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            if ate_result > max_ate_test_f1:\n",
    "                max_ate_test_f1 = ate_result\n",
    "            current_apc_test_acc = apc_result['max_apc_test_acc']\n",
    "            current_apc_test_f1 = apc_result['max_apc_test_f1']\n",
    "            current_ate_test_f1 = round(ate_result, 2)\n",
    "    #if epoch % 5 == 0:    \n",
    "    print('******')\n",
    "    print('Epoch %s'%epoch)\n",
    "    print(f'APC_test_acc:{current_apc_test_acc}(max: {max_apc_test_acc})')\n",
    "    print(f'APC_test_f1:{current_apc_test_f1}(max: {max_apc_test_f1})')\n",
    "    print(f'ATE_test_f1:{current_ate_test_f1}(max:{max_ate_test_f1})')\n",
    "    print('******')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d2b44-ec53-4258-8045-71e6054382bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"./temp/\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "label_map = {i : label for i, label in enumerate(LABEL_LIST,1)}\n",
    "model_config = {\n",
    "    \"bert_model\": PRETRAINED_BERT_MODEL,\n",
    "    \"do_lower\": True,\n",
    "    \"max_seq_length\": MAX_SEQUENCE_LENGTH,\n",
    "    \"num_labels\": len(LABEL_LIST)+1,\n",
    "    \"label_map\": label_map\n",
    "}\n",
    "json.dump(model_config, open(os.path.join(SAVE_PATH, \"config.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e8d75-0102-49f3-9e40-66e3f30a7111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e97a2-9562-4424-80e4-e2fe2131a6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepL",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
